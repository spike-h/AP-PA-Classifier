{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset Creator",
      "provenance": [],
      "collapsed_sections": [
        "0No-SI7bGxFN",
        "YhBQihc69OqV",
        "qVVGY7_JfAiY",
        "Vq1fjygmokq8",
        "rT3siDHbb6Vp",
        "DKSclpJAe8G7",
        "eVZhVfN2fO2U"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spike-h/AP-PA-Classifier/blob/main/Dataset_Creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSccms5KbUNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69011e5c-a583-4537-d852-97921157d08d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUeddQizbU-1"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2GQzSrPGuA1"
      },
      "source": [
        "##Pips (restart runtime after running)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISptNvTabNba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf89b544-878b-44dd-b10b-a01d9fb12188"
      },
      "source": [
        "'''\n",
        " importing from https://github.com/Engineero/tf_sprinkles\n",
        "'''\n",
        "!pip install tf_sprinkles &> /dev/null\n",
        "!pip install pydicom &> /dev/null\n",
        "!pip install pylibjpeg pylibjpeg-libjpeg  &> /dev/null\n",
        "!pip install python-gdcm &> /dev/null\n",
        "!pip install -q tensorflow-io &> /dev/null\n",
        "!pip install numpy --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 309 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0No-SI7bGxFN"
      },
      "source": [
        "##imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJpJOKOGG5-",
        "outputId": "ea750791-37f4-47b2-e1b6-8ec1af07491b"
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tf_sprinkles import Sprinkles\n",
        "import pydicom\n",
        "from tqdm import tqdm\n",
        "import pylibjpeg\n",
        "import gdcm\n",
        "import tensorflow_io as tfio\n",
        "import time\n",
        "# import tracemalloc\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.6.0\n",
            "Hub version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onDAy5-lpCE9"
      },
      "source": [
        "colabDir = '/content/drive/MyDrive/AP PA Stuff/Datasets/Original/'\n",
        "tensorflowDir = '/content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxi1_0x7WadX"
      },
      "source": [
        "#Creating tf.data.Dataset Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BZ9VuWc-It7"
      },
      "source": [
        "##Helper/Starter functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUHBLeTEEcZV"
      },
      "source": [
        "class_distributions = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIMUy9Awcd2w"
      },
      "source": [
        "#Sprinkles augmentation setup\n",
        "\n",
        "#Set sprinkles to make 50-100 square holes in the image with side length of 10 pixels 30% ofthe time\n",
        "class RandomSprinkles(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "  def call(self, image):\n",
        "    if random.random() < .3:\n",
        "      sprinkles = Sprinkles(num_holes=random.randint(50,100), side_length=10)\n",
        "      return sprinkles(image)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCZaec9y2t22"
      },
      "source": [
        "#Helper functions to create and augment the tf.data.Datasets\n",
        "IMG_SIZE = 256 #IF CHANGE HERE THEN ALSO CHANGE IN TRAIN MODEL.py\n",
        "BATCH_SIZE = 16 #IF CHANGE HERE THEN ALSO CHANGE IN TRAIN MODEL.py\n",
        "rngJesus = 8679309 #set seed for consistency when running but doesnt affect sprinkles soooooo maybe pointless\n",
        "\n",
        "def read_image(filename, label, bmp=None):\n",
        "  image_string = tf.io.read_file(filename)\n",
        "  image_decoded = tf.image.decode_image(image_string, channels=3, expand_animations=False)\n",
        "  image = tf.cast(image_decoded, tf.float32) / 255.0\n",
        "  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "  return image, label\n",
        "\n",
        "def read_dicom_image(dcmFile, label):\n",
        "  # image_decoded = tf.convert_to_tensor(dcmFile)\n",
        "  # image_decoded = tf.expand_dims(image_decoded, axis=-1)\n",
        "  image = tf.image.grayscale_to_rgb(dcmFile)\n",
        "  print('a')\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "  print('b')\n",
        "  # image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "  return image, label\n",
        "\n",
        "preprocessing_model = tf.keras.Sequential([\n",
        "  RandomSprinkles(),\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\", seed=rngJesus),\n",
        "  tf.keras.layers.RandomContrast(factor=0.1, seed=rngJesus),\n",
        "  tf.keras.layers.RandomTranslation(0, 0.1, fill_mode='constant', seed=rngJesus),\n",
        "  tf.keras.layers.RandomTranslation(0.1, 0, fill_mode='constant', seed=rngJesus),\n",
        "  tf.keras.layers.RandomRotation(.069, fill_mode='constant', seed=rngJesus)\n",
        "])\n",
        "# preprocessing_model.add(\n",
        "#   tf.keras.layers.RandomRotation(.111, fill_mode='constant', seed=rngJesus)),\n",
        "# preprocessing_model.add(\n",
        "#   tf.keras.layers.RandomTranslation(0, 0.1, fill_mode='constant', seed=rngJesus))\n",
        "# preprocessing_model.add(\n",
        "#   tf.keras.layers.RandomTranslation(0.1, 0, fill_mode='constant', seed=rngJesus))\n",
        "# # preprocessing_model.add(\n",
        "# #   tf.keras.layers.RandomZoom(0.05, 0.05, fill_mode='constant', seed=rngJesus))\n",
        "# preprocessing_model.add(\n",
        "#   tf.keras.layers.RandomFlip(\"horizontal\", seed=rngJesus))\n",
        "# preprocessing_model.add(RandomSprinkles())\n",
        "\n",
        "#Function to show image\n",
        "def show_image(dataset):\n",
        "  dataset = dataset.take(1)\n",
        "  image, label = next(iter(dataset))\n",
        "  image = image[0,:,:,:]\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "#Training/Validation/Test Split\n",
        "def training_split(dataset, training_prop=0.8, validation_prop=0.1, testing_prop=0.1, train=False):\n",
        "  print('c')\n",
        "  dataset_size = dataset.cardinality().numpy()\n",
        "  print('d')\n",
        "  train_size = int(training_prop * dataset_size)\n",
        "  print('e')\n",
        "  val_size = int(validation_prop * dataset_size)\n",
        "  print('f')\n",
        "  test_size = int(testing_prop * dataset_size)\n",
        "  print('g')\n",
        "  \n",
        "  if not train:\n",
        "    train_ds = dataset.take(train_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    #.prefetch(tf.data.AUTOTUNE)   \n",
        "    print('h') \n",
        "    val_ds = dataset.skip(train_size).take(val_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    print('i')\n",
        "    test_ds = dataset.skip(train_size).skip(val_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    print('j')\n",
        "  \n",
        "  else:\n",
        "    train_ds = dataset.take(train_size)\n",
        "    #.prefetch(tf.data.AUTOTUNE)   \n",
        "    print('h') \n",
        "    val_ds = dataset.skip(train_size).take(val_size)\n",
        "    print('i')\n",
        "    test_ds = dataset.skip(train_size).skip(val_size)\n",
        "    print('j')\n",
        "  \n",
        "  return train_ds, val_ds, test_ds\n",
        "\n",
        "def normalize(image, label):\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "  return image, label"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNF2CKvdNtl3"
      },
      "source": [
        "While many of the cells will look mostly the same, the difference lies in the file paths and metadata csv fields (and some variable names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhBQihc69OqV"
      },
      "source": [
        "##Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB8IG1pCbzp1"
      },
      "source": [
        "def random_shard(shard_size, dataset_size):\n",
        "  num_shards = -(-dataset_size // shard_size)  # Ceil division.\n",
        "  offsets = np.linspace(\n",
        "      0, dataset_size, num=num_shards, endpoint=False, dtype=np.int64)\n",
        "\n",
        "  def _random_shard(dataset):\n",
        "    sharded_dataset = tf.data.Dataset.from_tensor_slices(offsets)\n",
        "    sharded_dataset = sharded_dataset.shuffle(num_shards)\n",
        "    sharded_dataset = sharded_dataset.map(\n",
        "        lambda offset: dataset.skip(offset).take(shard_size))\n",
        "    print(sharded_dataset)\n",
        "    return sharded_dataset\n",
        "\n",
        "  return _random_shard\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxzhZXTbQ_8W"
      },
      "source": [
        "import gc\n",
        "count = 0\n",
        "def read_training_dataset(file_name, label):\n",
        "  image = tf.io.read_file(file_name)\n",
        "  if label == 999.0:\n",
        "    label = tfio.image.decode_dicom_data(image, tags=[tfio.image.dicom_tags.SeriesDescription])[0]\n",
        "    image = tfio.image.decode_dicom_image(image)[0,:,:,:]\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    if tf.strings.regex_full_match(label, \"(.*AP.*)\"):\n",
        "      label = 0.0\n",
        "    elif tf.strings.regex_full_match(label, \"(.*PA.*)\"):\n",
        "      label = 1.0\n",
        "    else:\n",
        "      label = 999.0\n",
        "      print(label)\n",
        "  else:\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "  time.sleep(.002)\n",
        "  gc.collect()\n",
        "  return image, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8js-YjFHiCM"
      },
      "source": [
        "def better_sharding(dataset, num_shards):\n",
        "  return tf.data.Dataset.from_tensor_slices([dataset.shard(num_shards, i) for i in range(0,num_shards)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmAf54gZSst"
      },
      "source": [
        "#TEST ON 13-24 FILE GROUPS AND KAGGLE TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTK_NmeUDisa"
      },
      "source": [
        "Kaggle_AP = 0\n",
        "Kaggle_PA = 0\n",
        "IEEE_PA = 0\n",
        "IEEE_AP = 0\n",
        "epoch_number = 9\n",
        "num_shards = 4\n",
        "\n",
        "def train_gen(half=None):\n",
        "  global file_count\n",
        "  global Kaggle_AP\n",
        "  global Kaggle_PA\n",
        "  global IEEE_AP\n",
        "  global IEEE_PA\n",
        "\n",
        "  dataDir = 'stage_2_train_images/'\n",
        "\n",
        "  #Getting dcm file paths from premade folders\n",
        "  #I manually Separated dcm into seperate folders bc google couldnt read 26k files in one line\n",
        "  file_paths = []\n",
        "  labels = []\n",
        "  dcm_dir_names = [str(i) for i in range(1,25)]\n",
        "  for name in dcm_dir_names:\n",
        "    dcm_dir = colabDir + dataDir + name + '/'\n",
        "    files = os.listdir(dcm_dir)\n",
        "    for file in files:\n",
        "      file_paths.append(dcm_dir + file)\n",
        "      labels.append(999.0)\n",
        "\n",
        "  # dataDir = 'stage_2_test_images/'\n",
        "  # dcm_files = os.listdir(colabDir + dataDir)\n",
        "  # for dcm in dcm_files:\n",
        "  #   file_paths.append(colabDir + dataDir + dcm)\n",
        "  #   labels.append('placeholder')\n",
        "\n",
        "  # ind=0\n",
        "  # if half:\n",
        "  #   half = 10000\n",
        "  # while ind<len(file_paths[:half]):\n",
        "  #   dcm = pydicom.dcmread(file_paths[ind])\n",
        "  #   image = dcm.pixel_array\n",
        "  #   image = tf.image.resize(tf.expand_dims(image, axis=-1), [IMG_SIZE, IMG_SIZE])\n",
        "  #   image = tf.image.grayscale_to_rgb(image)\n",
        "  #   label = dcm.SeriesDescription\n",
        "  #   if 'AP' in label:\n",
        "  #     Kaggle_AP += 1\n",
        "  #     label = 0.0\n",
        "  #   elif 'PA' in label:\n",
        "  #     Kaggle_PA += 1\n",
        "  #     label = 1.0\n",
        "  #   yield image, label\n",
        "  #   ind+=1\n",
        "\n",
        "  dataDir = 'covid-chestxray-dataset-master/covid-chestxray-dataset-master/'\n",
        "\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' #train metadata path\n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['view'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  IEEE_file_paths = metadata['filename'].values\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(IEEE_file_paths):\n",
        "    IEEE_file_paths[ind] = colabDir + dataDir + 'images/' + path\n",
        "  file_paths.extend(IEEE_file_paths)\n",
        "\n",
        "  #Get AP/PA label\n",
        "  IEEE_labels = metadata['view'].values\n",
        "  labels.extend(IEEE_labels)\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  # for ind, label in enumerate(labels):\n",
        "\n",
        "  #oop cheXpert time (run that cell first)\n",
        "  file_paths.extend(cheXpert_train_files_and_labels[0])\n",
        "  labels.extend(cheXpert_train_files_and_labels[1])\n",
        "\n",
        "  for ind, label in enumerate(labels):\n",
        "    if label == 999.0 or label == 0.0 or label == 1.0:\n",
        "      continue\n",
        "    elif 'AP' in label:\n",
        "      labels[ind] = 0.0\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1.0\n",
        "\n",
        "  zipped = list(zip(file_paths, labels))\n",
        "  random.shuffle(zipped)\n",
        "\n",
        "  ind=0\n",
        "  while ind<len(zipped):\n",
        "    # image = tf.io.read_file(file_paths[ind])\n",
        "    # image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    # label = labels[ind]\n",
        "    yield zipped[ind][0], zipped[ind][1]\n",
        "    ind+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJbjJw_WBTeO",
        "outputId": "9df2ce52-d903-443b-a8c7-f65e30b716c4"
      },
      "source": [
        "def create_train():\n",
        "  dataset = tf.data.Dataset.from_generator(train_gen, output_signature=(tf.TensorSpec(shape=(), dtype=tf.string), tf.TensorSpec(shape=(), dtype=tf.float32)))\n",
        "  dataset = dataset.apply(tf.data.experimental.assert_cardinality(47466)) #26684(kaggle) + 782(IEEE) + 20000(CheXpert)\n",
        "  dataset_length = dataset.cardinality().numpy()\n",
        "  buffer_size = int(.25 * dataset_length)\n",
        "  # shard_size = -(-dataset_size // num_shards)\n",
        "  \n",
        "  # shuffled_shards = []\n",
        "  # for i in shards:\n",
        "  #     subshard = shard(i)\n",
        "  #     subshard = subshard.interleave(lambda x: x, cycle_length=4, block_length=1)\n",
        "  #     shuffled_shards.append(subshard.shuffle(buffer_size=buffer_size))\n",
        "  # shuffled_shards = tf.data.Dataset.from_tensor_slices(shuffled_shards)\n",
        "  # dataset = shuffled_shards.interleave(lambda x: x, cycle_length=4, block_length=1)\n",
        "  # dataset = dataset.shuffle(num_shards)\n",
        "  # print(dataset)\n",
        "  dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "  dataset = dataset.map(read_training_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  print(dataset)\n",
        "  dataset = dataset.map(normalize)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=.8, validation_prop=.2, testing_prop=0, train=True)\n",
        "  train_ds = train_ds.map(lambda images, labels: (preprocessing_model(images), labels))\n",
        "  train_ds = train_ds.batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
        "  valid_ds = valid_ds.batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
        "  # test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset_length, train_ds, valid_ds, test_ds\n",
        "dataset_length, train_ds, valid_ds, test_ds = create_train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "999.0\n",
            "<ParallelMapDataset shapes: ((256, 256, 3), ()), types: (tf.float32, tf.float32)>\n",
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "b7Pa6RjN9YJm",
        "outputId": "21454021-817f-4603-f1a1-63c41a7a757a"
      },
      "source": [
        "'''\n",
        "def Construct_IEEE(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating ieee8023 Dataset (Covid-19 Image Data Collection)\n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='Ieee8023 Dataset'\n",
        "  dataDir = 'covid-chestxray-dataset-master/covid-chestxray-dataset-master/'\n",
        "\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' #train metadata path\n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['view'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  file_paths = metadata['filename'].values\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir + dataDir + 'images/' + path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  labels = metadata['view'].values\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "IEEE_ds = Construct_IEEE(.8,.2,0,augment=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef Construct_IEEE(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \\n  # Creating ieee8023 Dataset (Covid-19 Image Data Collection)\\n  # returns list: [train_ds(augmented), valid_ds, test_ds]\\n  # augment used to check if should apply random flip, rotation, etc.\\n  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\\n  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\\n\\n  name ='Ieee8023 Dataset'\\n  dataDir = 'covid-chestxray-dataset-master/covid-chestxray-dataset-master/'\\n\\n  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' #train metadata path\\n  metadata = pd.read_csv(METADATA_CSV_PATH)\\n\\n  #Only taking the rows that contain a value for view position\\n  metadata = metadata[metadata['view'].str.contains('AP|PA', case=False, regex=True, na=False)]\\n\\n  #Getting image file paths from csv\\n  file_paths = metadata['filename'].values\\n\\n  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\\n  #Change file paths to work with colab\\n  for ind, path in enumerate(file_paths):\\n    file_paths[ind] = colabDir + dataDir + 'images/' + path\\n\\n  #Get AP/PA label\\n  labels = metadata['view'].values\\n\\n  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\\n  AP_Count = 0\\n  PA_Count = 0\\n  for ind, label in enumerate(labels):\\n    if 'AP' in label:\\n      labels[ind] = 0\\n      AP_Count += 1\\n    elif 'PA' in label:\\n      labels[ind] = 1\\n      PA_Count += 1\\n  class_distributions[name] = {'AP':AP_Count,\\n                               'PA':PA_Count,\\n                               'Total': [f'Total Images = {AP_Count+PA_Count}',\\n                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\\n                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\\n\\n  labels = np.asarray(labels).astype('float32')\\n\\n  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\\n  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\\n\\n  if NumImages:\\n    dataset = dataset.take(NumImages)\\n\\n  dataset = dataset.map(read_image)\\n  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\\n\\n  #If data is used for training then give option to augment the data\\n  if augment:\\n    train_ds = train_ds.map(lambda images, labels:\\n              (preprocessing_model(images), labels))\\n    \\n  return [train_ds, valid_ds, test_ds]\\n\\nIEEE_ds = Construct_IEEE(.8,.2,0,augment=True)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVVGY7_JfAiY"
      },
      "source": [
        "###KAGGLE Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VSMqR_YySf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "9259504d-2f64-42cc-b554-a13458a490a6"
      },
      "source": [
        "'''\n",
        "def Construct_KaggleTrain(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating Kaggle Train Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='Kaggle Train Dataset'\n",
        "  # dataDir = 'stage_2_train_images/'\n",
        "\n",
        "  #Getting dcm file paths from premade folders\n",
        "  #I manually Separated dcm into seperate folders bc google couldnt read 26k files in one line\n",
        "  # file_paths = []\n",
        "  # dcm_dir_names = [str(i) for i in range(1,2)]\n",
        "  # for name in dcm_dir_names:\n",
        "  #   dcm_dir = colabDir + dataDir + name + '/'\n",
        "  #   files = os.listdir(dcm_dir)\n",
        "  #   for file in files:\n",
        "  #     file_paths.append(dcm_dir + file)\n",
        "\n",
        "  \n",
        "  #Split file_paths list into 500 file chunks to avoid ram overload when converting to tensor array\n",
        "  # file_groups = [file_paths[i:i + 500] for i in range(0, len(file_paths), 500)]\n",
        "  # ds_list = []\n",
        "\n",
        "  # for group in tqdm(file_groups):\n",
        "  # dcm_pixel_arrays = []\n",
        "  # labels = []\n",
        "  # for path in file_paths:\n",
        "  #   dcm = pydicom.dcmread(path)\n",
        "  #   image = dcm.pixel_array\n",
        "  #   image = tf.image.resize(tf.expand_dims(image, axis=-1), [IMG_SIZE, IMG_SIZE])\n",
        "  #   dcm_pixel_arrays.append(image)\n",
        "  #   labels.append(dcm.SeriesDescription)\n",
        "\n",
        "  # for ind, label in enumerate(labels):\n",
        "  #   if 'AP' in label:\n",
        "  #     labels[ind] = 0.0\n",
        "  #     AP_Count += 1\n",
        "  #   elif 'PA' in label:\n",
        "  #     labels[ind] = 1.0\n",
        "  #     PA_Count += 1\n",
        "        \n",
        "  dataset = tf.data.Dataset.from_generator(kaggle_train_gen, output_shapes=((256, 256, 1), ()), output_types=(tf.float32, tf.float32))\n",
        "\n",
        "  # dcm_pixel_arrays = []\n",
        "  # labels = []\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  # if NumImages:\n",
        "  #   dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_dicom_image)\n",
        "  # print(3)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "  # class_distributions[name] = {'AP':AP_Count,\n",
        "  #                              'PA':PA_Count,\n",
        "  #                              'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "  #                                        f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "  #                                        f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "  # print(4)\n",
        "  # # train_ds2, valid_ds2, test_ds2 = training_split(dataset, training_prop=0, validation_prop=0, testing_prop=1)\n",
        "\n",
        "  # #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "  #   print(5)\n",
        "\n",
        "  return [train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), valid_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)]\n",
        "\n",
        "Kaggle_Train_ds = Construct_KaggleTrain(.8,.2,0,augment=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef Construct_KaggleTrain(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \\n  # Creating Kaggle Train Dataset \\n  # returns list: [train_ds(augmented), valid_ds, test_ds]\\n  # augment used to check if should apply random flip, rotation, etc.\\n  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\\n  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\\n\\n  name ='Kaggle Train Dataset'\\n  # dataDir = 'stage_2_train_images/'\\n\\n  #Getting dcm file paths from premade folders\\n  #I manually Separated dcm into seperate folders bc google couldnt read 26k files in one line\\n  # file_paths = []\\n  # dcm_dir_names = [str(i) for i in range(1,2)]\\n  # for name in dcm_dir_names:\\n  #   dcm_dir = colabDir + dataDir + name + '/'\\n  #   files = os.listdir(dcm_dir)\\n  #   for file in files:\\n  #     file_paths.append(dcm_dir + file)\\n\\n  \\n  #Split file_paths list into 500 file chunks to avoid ram overload when converting to tensor array\\n  # file_groups = [file_paths[i:i + 500] for i in range(0, len(file_paths), 500)]\\n  # ds_list = []\\n\\n  # for group in tqdm(file_groups):\\n  # dcm_pixel_arrays = []\\n  # labels = []\\n  # for path in file_paths:\\n  #   dcm = pydicom.dcmread(path)\\n  #   image = dcm.pixel_array\\n  #   image = tf.image.resize(tf.expand_dims(image, axis=-1), [IMG_SIZE, IMG_SIZE])\\n  #   dcm_pixel_arrays.append(image)\\n  #   labels.append(dcm.SeriesDescription)\\n\\n  # for ind, label in enumerate(labels):\\n  #   if 'AP' in label:\\n  #     labels[ind] = 0.0\\n  #     AP_Count += 1\\n  #   elif 'PA' in label:\\n  #     labels[ind] = 1.0\\n  #     PA_Count += 1\\n        \\n  dataset = tf.data.Dataset.from_generator(kaggle_train_gen, output_shapes=((256, 256, 1), ()), output_types=(tf.float32, tf.float32))\\n\\n  # dcm_pixel_arrays = []\\n  # labels = []\\n\\n  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\\n\\n  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\\n\\n  # if NumImages:\\n  #   dataset = dataset.take(NumImages)\\n\\n  dataset = dataset.map(read_dicom_image)\\n  # print(3)\\n  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\\n  # class_distributions[name] = {'AP':AP_Count,\\n  #                              'PA':PA_Count,\\n  #                              'Total': [f'Total Images = {AP_Count+PA_Count}',\\n  #                                        f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\\n  #                                        f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\\n  # print(4)\\n  # # train_ds2, valid_ds2, test_ds2 = training_split(dataset, training_prop=0, validation_prop=0, testing_prop=1)\\n\\n  # #If data is used for training then give option to augment the data\\n  if augment:\\n    train_ds = train_ds.map(lambda images, labels:\\n              (preprocessing_model(images), labels))\\n  #   print(5)\\n\\n  return [train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), valid_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)]\\n\\nKaggle_Train_ds = Construct_KaggleTrain(.8,.2,0,augment=True)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sVpwI_KGDT0q",
        "outputId": "53ddaad1-829d-4d57-adf2-e4ba6f9f1b93"
      },
      "source": [
        "'''\n",
        "Kaggle_Train_ds\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nKaggle_Train_ds\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3aITjTa82t1"
      },
      "source": [
        "# Kaggle_Train_Snapshot = Kaggle_Train_ds[0].snapshot(tensorflowDir + 'Kaggle Train Dataset')\n",
        "# Kaggle_Train_Snapshot = tf.data.Dataset.snapshot(tensorflowDir + 'Kaggle Train Dataset')\n",
        "# Kaggle_Valid_Snapshot = Kaggle_Train_ds[1].snapshot(tensorflowDir + 'Kaggle Valid Dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7VOa6m3fKZy"
      },
      "source": [
        "#returns 2 lists with the second one being a test set in case I wanna flip it so i train on the kaggle test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4v4afK09eC-"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1fjygmokq8"
      },
      "source": [
        "###CheXpert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0NxEHpuZt3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260aa3e0-87d0-4332-981e-ce71d2939b46"
      },
      "source": [
        "cheXpert_train_files_and_labels = [[],[]]\n",
        "def Construct_CheXpert(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating CheXpert Dataset\n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc. to training dataset\n",
        "  # int NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # float TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='CheXpert Dataset'\n",
        "  dataDir = 'CheXpert-v1.0-small/'\n",
        "\n",
        "  #/content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train.csv\n",
        "  #/content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/valid.csv\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'train.csv' #train metadata path\n",
        "  METADATA_CSV_PATH2 = colabDir + dataDir + 'valid.csv' #validation metadata path\n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "  metadata2 = pd.read_csv(METADATA_CSV_PATH2)\n",
        "  metadata = pd.concat([metadata, metadata2], ignore_index=True)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['AP/PA'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  file_paths = list(metadata['Path'].values)\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir + path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  labels = list(metadata['AP/PA'].values)\n",
        "  #Not really needed for CheXpert but if ap/pa label has extra words then it'll cut that out & also gets counts of each label\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  ap_indexes = []\n",
        "  pa_indexes = []\n",
        "  cheXpert_train_indexes = []\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0.0\n",
        "      ap_indexes.append(ind)\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1.0\n",
        "      pa_indexes.append(ind)\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        " \n",
        " \n",
        "  cheXpert_train_indexes.extend(ap_indexes[:10000])\n",
        "  cheXpert_train_indexes.extend(pa_indexes[:10000])\n",
        "  for index in sorted(cheXpert_train_indexes, reverse=True):\n",
        "    cheXpert_train_files_and_labels[0].append(file_paths[index])\n",
        "    cheXpert_train_files_and_labels[1].append(labels[index])\n",
        "    del labels[index]\n",
        "    del file_paths[index]\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  cheXpert_ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  cheXpert_ds = cheXpert_ds.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    cheXpert_ds = cheXpert_ds.take(NumImages)\n",
        "\n",
        "  cheXpert_ds = cheXpert_ds.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(cheXpert_ds, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "  \n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "  # do dataset.batch(BATCHSIZE)\n",
        "\n",
        "cheXpert_ds = Construct_CheXpert(augment=False, TrainSplit=0, ValSplit=0, TestSplit=1)\n",
        "# tf.data.experimental.save(cheXpert_ds, tensorflowDir+name)\n",
        "# new_dataset = tf.data.experimental.load(tensorflowDir+name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT3siDHbb6Vp"
      },
      "source": [
        "###CX-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokWshXnWFYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3831ef19-ac99-401a-8f35-c0a4b8a82014"
      },
      "source": [
        "weird=[]\n",
        "def Construct_CXNet(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating CX-Net Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='CX-Net Dataset'\n",
        "  dataDir = 'covid_cxnet_dataset/'\n",
        "\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' #train metadata path\n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['view'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "  metadata = metadata[metadata['filename'].str.contains('jpg|png|jpeg', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  file_paths2 = metadata['filename'].values\n",
        "  other_paths = os.listdir(colabDir + dataDir + 'covid19/')\n",
        "  temp = []\n",
        "  file_paths = []\n",
        "\n",
        "  for file_path in file_paths2:\n",
        "    temp.append(file_path.split('.')[0])\n",
        "  for file_path in other_paths:\n",
        "    file_name = file_path.split('.')\n",
        "    if file_name[0] in temp:\n",
        "      weird.append(file_name[1])\n",
        "      file_paths.append(file_path)\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir + dataDir + 'covid19/' + path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  labels = metadata['view'].values\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "CXNet_ds = Construct_CXNet(0,0,1,augment=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4LYL6IaCFa"
      },
      "source": [
        "###ActualMed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhfH0jaJaMJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c65436-b82d-4910-a573-7a41eb1cc804"
      },
      "source": [
        "def Construct_ActualMed(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating ActualMed Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='ActualMed Dataset'\n",
        "  dataDir = 'Actualmed-COVID-chestxray-dataset-master/'\n",
        "\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' #train metadata path\n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['view'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  file_paths = metadata['imagename'].values\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir + dataDir + 'images/' + path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  labels = metadata['view'].values\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "ActualMed_ds = Construct_ActualMed(0,0,1,augment=False)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqIRNbC4bJy0"
      },
      "source": [
        "###Shenzhen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdRcwBhsbv4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb0598b-6a26-4577-988f-1b5c863f699b"
      },
      "source": [
        "def Construct_Shenzhen(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating Shenzhen Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  #Shenzhen dataset only has PA images so no metadata.csv checks\n",
        "\n",
        "  name ='Shenzhen Dataset'\n",
        "  dataDir = 'ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png/'\n",
        "\n",
        "  #Getting image file paths\n",
        "  file_paths = os.listdir(colabDir + dataDir)\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir+dataDir+path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  #AP:0, PA:1\n",
        "  labels = [1 for i in file_paths]\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = len(labels)\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "Shenzhen_ds = Construct_Shenzhen(0,0,1,augment=False)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hsRAvV0d5oE"
      },
      "source": [
        "###Montgomery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmzM83M5d-ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b651792-69bd-4021-ec0d-2ae3d69b275e"
      },
      "source": [
        "def Construct_Montgomery(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating Montgomery Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  #Montgomery dataset only has PA images so no metadata.csv checks\n",
        "\n",
        "  name ='Montgomery Dataset'\n",
        "  dataDir = 'NLM-MontgomeryCXRSet/MontgomerySet/CXR_png/'\n",
        "\n",
        "  #Getting image file paths\n",
        "  file_paths = os.listdir(colabDir + dataDir)\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir+dataDir+path\n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(file_paths):\n",
        "    file_paths[ind] = colabDir + dataDir + 'images/' + path\n",
        "\n",
        "  #Get AP/PA label\n",
        "  #AP:0, PA:1\n",
        "  labels = [1 for i in file_paths]\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = len(labels)\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "Montgomery_ds = Construct_Montgomery(0,0,1,augment=False)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKSclpJAe8G7"
      },
      "source": [
        "###RICORD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG22X20wH6WM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a47fcaf-8d7d-470e-baeb-30b9a3b9b6cf"
      },
      "source": [
        "def Construct_RICORD(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating RICORD Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='RICORD Dataset'\n",
        "  dataDir = 'manifest-1610656454899/'\n",
        "\n",
        "  METADATA_CSV_PATH = colabDir + dataDir + 'metadata.csv' \n",
        "  metadata = pd.read_csv(METADATA_CSV_PATH)\n",
        "\n",
        "  #Only taking the rows that contain a value for view position\n",
        "  metadata = metadata[metadata['Series Description'].str.contains('AP|PA', case=False, regex=True, na=False)]\n",
        "\n",
        "  #Getting image file paths from csv\n",
        "  dcm_dirs = metadata['File Location'].values \n",
        "\n",
        "  #Colab file path example: /content/drive/MyDrive/AP PA Stuff/Datasets/Original/CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "  #Change file paths to work with colab\n",
        "  for ind, path in enumerate(dcm_dirs):\n",
        "    path = path[2:] + '/'\n",
        "    path = path.replace('\\\\', '/')\n",
        "    dcm_dirs[ind] = colabDir + dataDir + path\n",
        "\n",
        "  file_paths = []\n",
        "  for dcm_dir in dcm_dirs:\n",
        "    dcm_path = os.listdir(dcm_dir)\n",
        "    for path in dcm_path:\n",
        "      file_paths.append(dcm_dir + path)\n",
        "\n",
        "  dcm_pixel_arrays = []\n",
        "  for path in tqdm(file_paths):\n",
        "      dcm = pydicom.dcmread(path)\n",
        "      img = dcm.pixel_array\n",
        "      img_2d = img.astype(float)\n",
        "      img_2d_scaled = (np.maximum(img_2d,0) / img_2d.max()) * 255.0\n",
        "      img_2d_scaled = np.uint8(img_2d_scaled)\n",
        "      image = tf.image.resize(tf.expand_dims(img_2d_scaled, axis=-1), [IMG_SIZE, IMG_SIZE])\n",
        "      dcm_pixel_arrays.append(image)\n",
        "\n",
        "  #Get AP/PA label\n",
        "  labels = []\n",
        "  imageCount = metadata['Number of Images']\n",
        "  preLabels = metadata['Series Description']\n",
        "  for count, label in zip(imageCount, preLabels):\n",
        "    for i in range(count):\n",
        "      labels.append(label)\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  print(len(dcm_pixel_arrays))\n",
        "  print(len(labels))\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dcm_pixel_arrays, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_dicom_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return [train_ds, valid_ds, test_ds]\n",
        "\n",
        "RICORD_ds = Construct_RICORD(0,0,1,augment=False)\n",
        "# show_image(RICORD_ds[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 694/694 [11:54<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "694\n",
            "694\n",
            "a\n",
            "b\n",
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZSp0b-O7y26"
      },
      "source": [
        "RICORD_test_ds = RICORD_ds[2].cache(tensorflowDir+'RICORD Dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVZhVfN2fO2U"
      },
      "source": [
        "###Kaggle Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-E2T5QiMK0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14626abc-7003-4e6e-a38c-85cca98b3159"
      },
      "source": [
        "def Construct_KaggleTest(TrainSplit, ValSplit, TestSplit, augment=False, NumImages=None): \n",
        "  # Creating Kaggle Test Dataset \n",
        "  # returns list: [train_ds(augmented), valid_ds, test_ds]\n",
        "  # augment used to check if should apply random flip, rotation, etc.\n",
        "  # NumImages used to determine how many images to include in database (if left blank then it takes all images)\n",
        "  # TrainSplit, ValSplit, TestSplit used to determine proportion of dataset for each split\n",
        "\n",
        "  name ='Kaggle Test Dataset'\n",
        "  dataDir = 'stage_2_test_images/'\n",
        "\n",
        "  #Getting dcm file paths from premade folders\n",
        "  #I manually Separated dcm into seperate folders bc google couldnt read 26k files in one line\n",
        "  file_paths = []\n",
        "  dcm_files = os.listdir(colabDir + dataDir)\n",
        "  for dcm in dcm_files:\n",
        "    file_paths.append(colabDir + dataDir + dcm)\n",
        "  # dcm_dir_names = [str(i) for i in range(1,2)]\n",
        "  # for name in dcm_dir_names:\n",
        "  #   dcm_dir = colabDir + dataDir + name + '/'\n",
        "  #   files = os.listdir(dcm_dir)\n",
        "  #   for file in files:\n",
        "  #     file_paths.append(dcm_dir + file)\n",
        "\n",
        "  dcm_pixel_arrays = []\n",
        "  labels = []\n",
        "  for path in tqdm(file_paths):\n",
        "      dcm = pydicom.dcmread(path)\n",
        "      image = dcm.pixel_array\n",
        "      image = tf.image.resize(tf.expand_dims(image, axis=-1), [IMG_SIZE, IMG_SIZE])\n",
        "      dcm_pixel_arrays.append(image)\n",
        "      labels.append(dcm.SeriesDescription)\n",
        "\n",
        "  #if ap/pa label has extra words then it'll cut that out & gets counts of each label & converts labels into 1 and 0\n",
        "  AP_Count = 0\n",
        "  PA_Count = 0\n",
        "  for ind, label in enumerate(labels):\n",
        "    if 'AP' in label:\n",
        "      labels[ind] = 0\n",
        "      AP_Count += 1\n",
        "    elif 'PA' in label:\n",
        "      labels[ind] = 1\n",
        "      PA_Count += 1\n",
        "  class_distributions[name] = {'AP':AP_Count,\n",
        "                               'PA':PA_Count,\n",
        "                               'Total': [f'Total Images = {AP_Count+PA_Count}',\n",
        "                                         f'AP Percentage = {AP_Count/(AP_Count+PA_Count):.3f}',\n",
        "                                         f'PA Percentage = {PA_Count/(AP_Count+PA_Count):.3f}']}\n",
        "\n",
        "  labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "  print(len(dcm_pixel_arrays))\n",
        "  print(len(labels))\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dcm_pixel_arrays, labels))\n",
        "  dataset = dataset.shuffle(buffer_size=100000, seed=rngJesus)\n",
        "\n",
        "  if NumImages:\n",
        "    dataset = dataset.take(NumImages)\n",
        "\n",
        "  dataset = dataset.map(read_dicom_image)\n",
        "  train_ds, valid_ds, test_ds = training_split(dataset, training_prop=TrainSplit, validation_prop=ValSplit, testing_prop=TestSplit)\n",
        "  # train_ds2, valid_ds2, test_ds2 = training_split(dataset, training_prop=.8, validation_prop=.2, testing_prop=0)\n",
        "\n",
        "  #If data is used for training then give option to augment the data\n",
        "  if augment:\n",
        "    train_ds = train_ds.map(lambda images, labels:\n",
        "              (preprocessing_model(images), labels))\n",
        "    \n",
        "  return train_ds, valid_ds, test_ds\n",
        "    \n",
        "  # return [train_ds2, valid_ds2, test_ds2]\n",
        "\n",
        "Kaggle_Test_ds = Construct_KaggleTest(0,0,1,augment=True)\n",
        "# show_image(RICORD_ds[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [18:19<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "3000\n",
            "a\n",
            "b\n",
            "c\n",
            "d\n",
            "e\n",
            "f\n",
            "g\n",
            "h\n",
            "i\n",
            "j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6KOtYAXfQ4d"
      },
      "source": [
        "#returns 2 lists with the second one being a train set in case I wanna flip it so i train on the kaggle test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRFsXgIdFd6q"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivxguypWazSx",
        "outputId": "e00cdaaa-554b-43f0-b9a5-7dcc0c747330"
      },
      "source": [
        "#test cell to see if model works with dataset\n",
        "model_handle = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2'\n",
        "# classifier = hub.load(model_handle)\n",
        "print(\"Building model with\", model_handle)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=(IMG_SIZE,IMG_SIZE) + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=True),\n",
        "    # tf.keras.layers.Dense(256),\n",
        "    # tf.keras.layers.Flatten(name=\"flatten\"),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
        "])\n",
        "model.build((None,)+(IMG_SIZE,IMG_SIZE)+(3,))\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1280)              117746848 \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1281      \n",
            "=================================================================\n",
            "Total params: 117,748,129\n",
            "Trainable params: 117,235,553\n",
            "Non-trainable params: 512,576\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59-ogqTMDNeD"
      },
      "source": [
        "#taken from https://ecode.dev/cnn-for-medical-imaging-using-tensorflow-2/\n",
        "from tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, BinaryAccuracy, Precision, Recall, AUC\n",
        "from tensorflow.keras.metrics import SpecificityAtSensitivity\n",
        "\n",
        "METRICS = [\n",
        "      TruePositives(name='tp'),\n",
        "      FalsePositives(name='fp'),\n",
        "      TrueNegatives(name='tn'),\n",
        "      FalseNegatives(name='fn'), \n",
        "      BinaryAccuracy(name='accuracy'),\n",
        "      Precision(name='precision'),\n",
        "      Recall(name='recall'),\n",
        "      AUC(name='auc'),\n",
        "      SpecificityAtSensitivity(sensitivity=0.8, name='sensitivity')\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ty_wZtlazSz"
      },
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "  metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTl6LPchTKEz"
      },
      "source": [
        "BetterDir = '/content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/'\n",
        "checkpoint_path = BetterDir + \"cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86nrk3d1Uye"
      },
      "source": [
        "class ClearStuff(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0u3769n4fKP",
        "outputId": "488b21d4-cab9-41e9-e1e8-d71156acbef8"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd1451a9a90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k04LX9OkazS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f905416-bc0b-4388-b6f7-053cb41e7704"
      },
      "source": [
        "steps_per_epoch = dataset_length*.8 // BATCH_SIZE\n",
        "validation_steps = dataset_length*.1 // BATCH_SIZE\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epoch_number, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_ds,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[cp_callback, ClearStuff()]).history \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "2373/2373 [==============================] - 2010s 822ms/step - loss: 0.0586 - tp: 19471.0000 - fp: 174.0000 - tn: 17906.0000 - fn: 417.0000 - accuracy: 0.9844 - precision: 0.9911 - recall: 0.9790 - auc: 0.9958 - sensitivity: 0.9989 - val_loss: 0.0518 - val_tp: 2443.0000 - val_fp: 17.0000 - val_tn: 2232.0000 - val_fn: 44.0000 - val_accuracy: 0.9871 - val_precision: 0.9931 - val_recall: 0.9823 - val_auc: 0.9965 - val_sensitivity: 0.9996\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 2/9\n",
            "2373/2373 [==============================] - 1959s 819ms/step - loss: 0.0579 - tp: 19438.0000 - fp: 161.0000 - tn: 17934.0000 - fn: 423.0000 - accuracy: 0.9846 - precision: 0.9918 - recall: 0.9787 - auc: 0.9956 - sensitivity: 0.9985 - val_loss: 0.0715 - val_tp: 2353.0000 - val_fp: 7.0000 - val_tn: 2285.0000 - val_fn: 91.0000 - val_accuracy: 0.9793 - val_precision: 0.9970 - val_recall: 0.9628 - val_auc: 0.9957 - val_sensitivity: 0.9991\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 3/9\n",
            "2373/2373 [==============================] - 1959s 818ms/step - loss: 0.0554 - tp: 19473.0000 - fp: 165.0000 - tn: 17911.0000 - fn: 407.0000 - accuracy: 0.9849 - precision: 0.9916 - recall: 0.9795 - auc: 0.9961 - sensitivity: 0.9987 - val_loss: 0.0517 - val_tp: 2382.0000 - val_fp: 14.0000 - val_tn: 2289.0000 - val_fn: 51.0000 - val_accuracy: 0.9863 - val_precision: 0.9942 - val_recall: 0.9790 - val_auc: 0.9967 - val_sensitivity: 0.9991\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 4/9\n",
            "2373/2373 [==============================] - 1961s 819ms/step - loss: 0.0561 - tp: 19488.0000 - fp: 157.0000 - tn: 17894.0000 - fn: 417.0000 - accuracy: 0.9849 - precision: 0.9920 - recall: 0.9791 - auc: 0.9960 - sensitivity: 0.9989 - val_loss: 0.1103 - val_tp: 2319.0000 - val_fp: 9.0000 - val_tn: 2266.0000 - val_fn: 142.0000 - val_accuracy: 0.9681 - val_precision: 0.9961 - val_recall: 0.9423 - val_auc: 0.9926 - val_sensitivity: 0.9991\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 5/9\n",
            "2373/2373 [==============================] - 1958s 818ms/step - loss: 0.0537 - tp: 19476.0000 - fp: 170.0000 - tn: 17919.0000 - fn: 391.0000 - accuracy: 0.9852 - precision: 0.9913 - recall: 0.9803 - auc: 0.9963 - sensitivity: 0.9987 - val_loss: 0.0606 - val_tp: 2403.0000 - val_fp: 9.0000 - val_tn: 2271.0000 - val_fn: 53.0000 - val_accuracy: 0.9869 - val_precision: 0.9963 - val_recall: 0.9784 - val_auc: 0.9943 - val_sensitivity: 0.9982\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 6/9\n",
            "2373/2373 [==============================] - 1961s 819ms/step - loss: 0.0539 - tp: 19418.0000 - fp: 156.0000 - tn: 17983.0000 - fn: 399.0000 - accuracy: 0.9854 - precision: 0.9920 - recall: 0.9799 - auc: 0.9963 - sensitivity: 0.9988 - val_loss: 0.0495 - val_tp: 2457.0000 - val_fp: 9.0000 - val_tn: 2231.0000 - val_fn: 39.0000 - val_accuracy: 0.9899 - val_precision: 0.9964 - val_recall: 0.9844 - val_auc: 0.9951 - val_sensitivity: 0.9996\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 7/9\n",
            "2373/2373 [==============================] - 1967s 821ms/step - loss: 0.0541 - tp: 19419.0000 - fp: 172.0000 - tn: 17969.0000 - fn: 396.0000 - accuracy: 0.9850 - precision: 0.9912 - recall: 0.9800 - auc: 0.9963 - sensitivity: 0.9987 - val_loss: 0.0601 - val_tp: 2440.0000 - val_fp: 9.0000 - val_tn: 2233.0000 - val_fn: 54.0000 - val_accuracy: 0.9867 - val_precision: 0.9963 - val_recall: 0.9783 - val_auc: 0.9937 - val_sensitivity: 0.9987\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 8/9\n",
            "2373/2373 [==============================] - 1975s 825ms/step - loss: 0.0506 - tp: 19534.0000 - fp: 163.0000 - tn: 17894.0000 - fn: 365.0000 - accuracy: 0.9861 - precision: 0.9917 - recall: 0.9817 - auc: 0.9966 - sensitivity: 0.9987 - val_loss: 0.0612 - val_tp: 2433.0000 - val_fp: 7.0000 - val_tn: 2236.0000 - val_fn: 60.0000 - val_accuracy: 0.9859 - val_precision: 0.9971 - val_recall: 0.9759 - val_auc: 0.9948 - val_sensitivity: 0.9996\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n",
            "Epoch 9/9\n",
            "2373/2373 [==============================] - 1996s 834ms/step - loss: 0.0514 - tp: 19518.0000 - fp: 165.0000 - tn: 17912.0000 - fn: 361.0000 - accuracy: 0.9861 - precision: 0.9916 - recall: 0.9818 - auc: 0.9967 - sensitivity: 0.9987 - val_loss: 0.0768 - val_tp: 2378.0000 - val_fp: 8.0000 - val_tn: 2261.0000 - val_fn: 89.0000 - val_accuracy: 0.9795 - val_precision: 0.9966 - val_recall: 0.9639 - val_auc: 0.9947 - val_sensitivity: 0.9991\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/MyDrive/AP PA Stuff/Datasets/Tensorflow/After/cp.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TAE7NRW8UlO"
      },
      "source": [
        "#before times \n",
        "epochs = [i for i in range(1,12)]\n",
        "accuracies = [0.9475,0.9675,0.9758,0.9758,0.9792,0.9811,0.9811,0.9818,0.9827,0.9828,0.9838]\n",
        "val_accuracies = [0.9719,0.9719,0.9688,0.9865,0.9683,0.9871,0.9867,0.9814,0.9884,0.9827,0.9829]\n",
        "losses = [0.1577,0.1132,0.0898,0.0863,0.0779,0.0732,0.0700,0.0669,0.0653,0.0653,0.0621]\n",
        "val_losses = [0.0967,0.1051,0.1346,0.0775,0.1336,0.0690,0.0527,0.0872,0.0581,0.0923,0.0619]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "yU0HxMW7HDkj",
        "outputId": "bb2c2a98-f317-474c-c836-6a295cd07c63"
      },
      "source": [
        "accuracies.extend(hist[\"accuracy\"])\n",
        "val_accuracies.extend(hist['val_accuracy'])\n",
        "losses.extend(hist['loss'])\n",
        "val_losses.extend(hist['val_loss'])\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(losses, 'b', label='Training Loss')\n",
        "plt.plot(val_losses, 'r', label='Validatiton Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(accuracies, 'b', label='Training Accuracy')\n",
        "plt.plot(val_accuracies,'r', label='Validatiton Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU1b3H8c8vIRD2HUV2FEEW2SLgVolbURTcRbFXFEWtS7WtW6vWq7W199alXKuWCtJFRWsVcUVUFJSiBGQXFTEqiOwkgRBIJr/7x/MQB5gkQ8xkkvB9v17zyjzn2X4ns/zmPMs55u6IiIjsLSXZAYiISPWkBCEiIjEpQYiISExKECIiEpMShIiIxKQEISIiMSUsQZhZBzObaWbLzWyZmf0sxjJmZuPNbKWZLTazAVHzLjWzz8PHpYmKU0REYrNE3QdhZm2Btu6+wMwaA/OBs9x9edQypwPXA6cDg4E/uftgM2sBZAEZgIfrDnT3LQkJVkRE9pGwFoS7r3X3BeHzPOAToN1ei40E/u6BuUCzMLH8GJjh7pvDpDADGJaoWEVEZF91qmInZtYZ6A98uNesdsA3UdOrw7LSymNtexwwDqBhw4YDe/ToUSkxi4gcCObPn7/R3VvHmpfwBGFmjYB/Aze6e25lb9/dJwATADIyMjwrK6uydyEiUmuZ2VelzUvoVUxmlkaQHJ5y9xdiLLIG6BA13T4sK61cRESqSCKvYjJgIvCJuz9YymLTgP8Kr2YaAuS4+1pgOnCqmTU3s+bAqWGZiIhUkUQeYjoW+AmwxMwWhmW/AjoCuPvjwGsEVzCtBPKBy8J5m83sXmBeuN497r45gbGKiMheEpYg3P19wMpZxoFrS5k3CZiUgNBEZD8VFhayevVqCgoKkh2KVFB6ejrt27cnLS0t7nWq5ComEanZVq9eTePGjencuTPB0WOpSdydTZs2sXr1arp06RL3eupqQ0TKVVBQQMuWLZUcaigzo2XLlvvdAlSCEJG4KDnUbBV5/ZQgREQkJiUIEan2Nm3aRL9+/ejXrx8HH3ww7dq1K5netWtXmetmZWVxww03lLuPY445plJifffddznjjDMqZVvJppPUIlLttWzZkoULg6vl7777bho1asQvf/nLkvlFRUXUqRP76ywjI4OMjIxy9zFnzpzKCbYWUQtCRGqkMWPGcPXVVzN48GBuueUWPvroI44++mj69+/PMcccw6effgrs+Yv+7rvv5vLLL2fo0KF07dqV8ePHl2yvUaNGJcsPHTqU8847jx49ejB69Gh293r92muv0aNHDwYOHMgNN9ywXy2FZ555hj59+tC7d29uvfVWACKRCGPGjKF379706dOHhx56CIDx48fTs2dPjjzySEaNGvXD/1kVpBaEiOyXG2+EhQvLX25/9OsHDz+8/+utXr2aOXPmkJqaSm5uLrNnz6ZOnTq89dZb/OpXv+Lf//73PuusWLGCmTNnkpeXR/fu3bnmmmv2uTfg448/ZtmyZRxyyCEce+yxfPDBB2RkZHDVVVcxa9YsunTpwkUXXRR3nN9++y233nor8+fPp3nz5px66qlMnTqVDh06sGbNGpYuXQrA1q1bAbj//vv58ssvqVevXklZMqgFISI11vnnn09qaioAOTk5nH/++fTu3ZubbrqJZcuWxVxn+PDh1KtXj1atWtGmTRvWrVu3zzKDBg2iffv2pKSk0K9fP7Kzs1mxYgVdu3YtuY9gfxLEvHnzGDp0KK1bt6ZOnTqMHj2aWbNm0bVrV1atWsX111/PG2+8QZMmTQA48sgjGT16NP/85z9LPXRWFdSCEJH9UpFf+onSsGHDkud33nknmZmZvPjii2RnZzN06NCY69SrV6/keWpqKkVFRRVapjI0b96cRYsWMX36dB5//HGee+45Jk2axKuvvsqsWbN4+eWXue+++1iyZElSEoVaECJSK+Tk5NCuXTBszOTJkyt9+927d2fVqlVkZ2cD8Oyzz8a97qBBg3jvvffYuHEjkUiEZ555hhNOOIGNGzdSXFzMueeey29/+1sWLFhAcXEx33zzDZmZmfzhD38gJyeHbdu2VXp94qEWhIjUCrfccguXXnopv/3tbxk+fHilb79+/fo8+uijDBs2jIYNG3LUUUeVuuzbb79N+/btS6b/9a9/cf/995OZmYm7M3z4cEaOHMmiRYu47LLLKC4uBuD3v/89kUiESy65hJycHNydG264gWbNmlV6feKRsDGpk0EDBokkxieffMIRRxyR7DCSbtu2bTRq1Ah359prr6Vbt27cdNNNyQ4rbrFeRzOb7+4xrwPWISYRkTj99a9/pV+/fvTq1YucnByuuuqqZIeUUDrEJCISp5tuuqlGtRh+KLUgREQkJiUIERGJSQlCRERiStg5CDObBJwBrHf33jHm3wyMjorjCKB1OB51NpAHRICi0s6wi4hI4iSyBTEZGFbaTHf/X3fv5+79gNuB99x9c9QimeF8JQeRA1xmZibTp0/fo+zhhx/mmmuuKXWdoUOHsvuy99NPPz1mn0Z33303f/zjH8vc99SpU1m+fHnJ9F133cVbb71VEkN+fn7c9SjL5MmTue666yplW5UlYQnC3WcBm8tdMHAR8EyiYhGRmu2iiy5iypQpe5RNmTIl7v6QXnvttQrfbLZ3grjnnns4+eSTgcpNENVR0s9BmFkDgpZGdLeLDrxpZvPNbFxyIhOR6uK8887j1VdfLRkcKDs7m2+//Zbjjz+ea665hoyMDHr16sVvfvObmOt37tyZjRs3AnDfffdx+OGHc9xxx5V0CQ7BPQ5HHXUUffv25dxzzyU/P585c+Ywbdo0br75Zvr168cXX3zBmDFjeP755xk/fjzffvstmZmZZGZmArG79IagK/Ff//rX9O3blyFDhsTsILA0Dz74IL1796Z37948HHaEtX37doYPH07fvn3p3bt3Sbcft912W0k34dHjZVRUdbgP4kzgg70OLx3n7mvMrA0ww8xWhC2SfYQJZBxAx44dEx+tyIEuCf19t2jRgkGDBvH6668zcuRIpkyZwgUXXICZcd9999GiRQsikQgnnXQSixcv5sgjj4y5nfnz5zNlyhQWLlxIUVERAwYMYODAgQCcc845XHnllQDccccdTJw4keuvv54RI0ZwxhlncN555+2xrRtuuIEHH3yQmTNn0qpVq1K79D7rrLPYvn07Q4YM4b777uOWW27hr3/9K3fccUe5/5b58+fz5JNP8uGHH+LuDB48mBNOOIFVq1ZxyCGH8OqrrwJBP1SbNm3ixRdfZMWKFZhZpXQTnvQWBDCKvQ4vufua8O964EVgUGkru/sEd89w94zWrVsnNFARSZ7ow0zRh5eee+45BgwYQP/+/Vm2bNkeh4P2Nnv2bM4++2waNGhAkyZNGDFiRMm8pUuXcvzxx9OnTx+eeuqpUrsLL01pXXoD1K1bt2RwoYEDB5Z0+Fee999/n7PPPpuGDRvSqFEjzjnnHGbPnk2fPn2YMWMGt956K7Nnz6Zp06Y0bdqU9PR0xo4dywsvvECDBg32K/5YktqCMLOmwAnAJVFlDYEUd88Ln58K3JOkEEVkb0nq73vkyJHcdNNNLFiwgPz8fAYOHMiXX37JH//4R+bNm0fz5s0ZM2YMBQUFFdr+mDFjmDp1Kn379mXy5Mm8++67lRZ7WloaZgZUTvfhhx9+OAsWLOC1117jjjvu4KSTTuKuu+7io48+4u233+b555/nkUce4Z133vlB+0lYC8LMngH+A3Q3s9VmNtbMrjazq6MWOxt40923R5UdBLxvZouAj4BX3f2NRMUpIjVDo0aNyMzM5PLLLy9pPeTm5tKwYUOaNm3KunXreP3118vcxo9+9COmTp3Kjh07yMvL4+WXXy6Zl5eXR9u2bSksLOSpp54qKW/cuDF5eXkxtxc9r7QuvX+I448/nqlTp5Kfn8/27dt58cUXOf744/n2229p0KABl1xyCTfffDMLFixg27Zt5OTkcPrpp/PQQw+xaNGiH7RvSGALwt3LvbzA3ScTXA4bXbYK6JuYqESkJrvooos4++yzSw419e3bl/79+9OjRw86dOjAscceW+b6AwYM4MILL6Rv3760adNmjy677733XgYPHkzr1q0ZPHhwyRf/qFGjuPLKKxk/fjzPP//8HtsbN24cw4YN45BDDmHmzJkxu/TeH5MnT2bq1Kkl03PnzmXMmDEMGhQcZb/iiivo378/06dP5+abbyYlJYW0tDQee+wx8vLyGDlyJAUFBbg7Dz744H7tOxZ19y0i5VJ337WDuvsWEZFKoQQhIiIxKUGISFxq0+HoA1FFXj8lCBEpV3p6Ops2bVKSqKHcnU2bNpGenr5f65V7FVN4N/OxwCHADmApkOXuxRUJVERqnvbt27N69Wo2bNiQ7FCkgtLT02nfvv1+rVNqgjCzTOA2oAXwMbAeSAfOAg41s+eBB9w9t8IRi0iNkJaWRpcuXZIdhlSxsloQpwNXuvvXe88wszoEYz2cwp6d7ImISC1RaoJw95vLmFcETC1tvoiI1HzxnIOoB5wLdI5e3t3VP5KISC0WT1cbLwE5wHxgZ2LDERGR6iKeBNHe3UsdOlRERGqneO6DmGNmfRIeiYiIVCvxtCCOA8aY2ZcEh5gMcHePPWSTiIjUCvEkiNMSHoWIiFQ75R5icvevgGYEY0efCTQLy0REpBYrN0GY2c+Ap4A24eOfZnZ9ogMTEZHkiucQ01hg8O5hQc3sDwRDif5fIgMTEZHkiucqJgMiUdORsExERGqxeBLEk8CHZna3md0NzAUmlreSmU0ys/VmtrSU+UPNLMfMFoaPu6LmDTOzT81spZndFmddRESkEpV7iMndHzSzdwkudwW4zN0/jmPbk4FHgL+Xscxsdz8jusDMUoE/E3QEuBqYZ2bT3H15HPsUEZFKUlZ3303cPdfMWgDZ4WP3vBbuvrmsDbv7LDPrXIGYBgEr3X1VuK8pwEhACUJEpAqV1YJ4mqBL7/lA9DBSFk53rYT9H21mi4BvgV+6+zKgHfBN1DKrgcGlbcDMxgHjADp27FgJIYmICJTd3fcZ4d9EjRKyAOjk7tvM7HSC7sO77e9G3H0CMAEgIyND4yGKiFSSeO6DeDuesv3l7rnuvi18/hqQZmatgDVAh6hF24dlIiJShco6B5EONABamVlzvr+0tQnBYaAfxMwOBta5u5vZIIJktQnYCnQzsy4EiWEUcPEP3Z+IiOyfss5BXAXcCBxCcB5id4LIJbg6qUxm9gwwlCDBrAZ+A6QBuPvjwHnANWZWBOwARrm7A0Vmdh0wHUgFJoXnJkREpApZ8J1cxgJm17t7jbhrOiMjw7OyspIdhohIjWFm8909I9a8eO6D+D8z6w30BNKjysu6v0FERGq4eMak/g3BoaKewGsE3X+/T9k3wImISA0XT1cb5wEnAd+5+2VAX6BpQqMSEZGkiydB7HD3YoKTx02A9ex5GaqIiNRC8XT3nWVmzYC/ElzNtI2gu28REanF4jlJ/dPw6eNm9gbQxN0XJzYsERFJtrJulBtQ1jx3X5CYkEREpDooqwXxQPg3HcgAFhHcLHckkAUcndjQREQkmUo9Se3ume6eCawFBrh7hrsPBPqjvpFERGq9eK5i6u7uS3ZPuPtS4IjEhSQiItVBPFcxLTazJ4B/htOjAZ2kFhGp5eJJEJcB1wA/C6dnAY8lLCIREakW4rnMtQB4KHyIiMgBoqzLXJ9z9wvMbAl7DjkKgLsfmdDIREQkqcpqQew+pHRGVQQiIiLVS1ljUq8N/35VdeGIiEh1UdYhpjxiHFoiuFnO3b1JwqISEZGkK6sF0bgqAxERkeolnhvlADCzNmbWcfcjjuUnmdl6M1tayvzRZrbYzJaY2Rwz6xs1LzssX2hmGkNURCQJyk0QZjbCzD4HvgTeA7KB1+PY9mRgWBnzvwROcPc+wL3AhL3mZ7p7v9LGShURkcSKpwVxLzAE+MzduxCMLje3vJXcfRawuYz5c9x9Szg5F2gfRywiIlJF4kkQhe6+CUgxsxR3n0nQu2tlGsuerRIH3jSz+WY2rqwVzWycmWWZWdaGDRsqOSwRkQNXPF1tbDWzRgRdbDxlZuuB7ZUVgJllEiSI46KKj3P3NWbWBphhZivCFsk+3H0C4eGpjIyMWFddiYhIBcTTghgJ5AM3AW8AXwBnVsbOzexI4AlgZNhKAcDd14R/1wMvAoMqY38iIhK/eBLEVUBbdy9y97+5+/joL/OKCq+EegH4ibt/FlXe0Mwa734OnArEvBJKREQSJ55DTI0JzgdsBp4F/uXu68pbycyeAYYCrcxsNfAbIA3A3R8H7gJaAo+aGUBReMXSQcCLYVkd4Gl3f2M/6yUiIj+Qucd32D48HHQhcC6w2t1PTmRgFZGRkeFZWbptQkQkXmY2v7TbCeK+UQ5YD3wHbALaVEZgIiJSfcVzo9xPzexd4G2CQ0JXqqtvEZHaL55zEB2AG919YaKDERGR6iOeEeVur4pARESketmfcxAiInIAUYIQEZGYlCBERCSmiowoB4BGlBMRqd3KHVHOzO4F1gL/IBhudDTQtkqiExGRpInnENMId3/U3fPcPdfdHyPowE9ERGqxeBLE9nB40FQzSzGz0VRid98iIlI9xZMgLgYuANaFj/PDMhERqcXiuVEuGx1SEhE54JSbIMysNXAl0Dl6eXe/PHFhiYhIssXTF9NLwGzgLSCS2HBERKS6iCdBNHD3WxMeiYiIVCvxnKR+xcxOT3gkIiJSrcSTIH5GkCR2mFmumeWZWW6iAxMRkeQqN0G4e2N3T3H3+u7eJJyOq5sNM5tkZuvNbGkp883MxpvZSjNbbGYDouZdamafh49L46+SiIhUhnjOQWBmzYFuQPruMnefFceqk4FHgL+XMv+0cLvdgMHAY8BgM2sB/AbIIOgPar6ZTXP3LfHEKyIiP1w8l7leQXCYqT2wEBgC/Ac4sbx13X2WmXUuY5GRwN/d3YG5ZtbMzNoCQ4EZ7r45jGEGMAx4prx9iohI5Yj3HMRRwFfungn0B7ZW0v7bAd9ETa8Oy0or34eZjTOzLDPL2rBhQyWFJSIi8SSIAncvADCzeu6+Auie2LDi5+4T3D3D3TNat26d7HBERGqNeBLEajNrBkwFZpjZS8BXlbT/NUCHqOn2YVlp5SIiUkXiuYrpbHff6u53A3cCE4GzKmn/04D/Cq9mGgLkuPtaYDpwqpk1D0+QnxqWiYhIFYnrKqbd3P29/VnezJ4hOOHcysxWE1yZlBZu63HgNeB0YCWQD1wWztscDlQ0L9zUPbtPWIuISNXYrwSxv9z9onLmO3BtKfMmAZMSEZeIiJQvnnMQIiJyAFKCEBGRmOK5US6P4G7maDlAFvALd1+ViMBERCS54jkH8TDBjWpPAwaMAg4FFhCcIxiaqOBERCR54jnENMLd/+Luee6e6+4TgB+7+7NA8wTHJyIiSRJPgsg3swvMLCV8XAAUhPP2PvQkIiK1RDwJYjTwE2A9sC58fomZ1QeuS2BsIiKSROWegwhPQp9Zyuz3KzccERGpLuK5iqk1cCXQOXp5d788cWGJiEiyxXMV00vAbOAtIJLYcEREpLqIJ0E0cPdbEx6JiIhUK/GcpH7FzE5PeCQiIlKtxDui3CtmtsPMcs0sz8xyEx2YiIgkVzxXMTWuikBERKR6KTVBmFkPd19hZgNizXf3BYkLS0REkq2sFsTPgXHAAzHmOXBiQiISEZFqodQE4e7jwr+ZVReOiIhUF3GNKGdmx7DvjXJ/T1BMIiJSDcRzJ/U/CLr3Xsj3N8o5UG6CMLNhwJ+AVOAJd79/r/kPAbtbKA2ANu7eLJwXAZaE87529xHl1kZERCpNPC2IDKBnOH503MwsFfgzcArBeBLzzGyauy/fvYy73xS1/PVA/6hN7HD3fvuzTxERqTzx3AexFDi4AtseBKx091XuvguYAowsY/mLgGcqsB8REUmAeFoQrYDlZvYRsHN3YRyHfNoB30RNrwYGx1rQzDoBXYB3oorTzSwLKALud/eppaw7juBqKzp27FhOSCIiEq94EsTdiQ6CYBjT5909ujPATu6+xsy6Au+Y2RJ3/2LvFcMR7iYAZGRkaAAjEZFKEs+d1O9VcNtrgA5R0+3DslhGAdfutd814d9VZvYuwfmJfRKEiIgkRqnnIMzs/fBvXtgHU+5+9sU0D+hmZl3MrC5BEpgWYz89CMa2/k9UWXMzqxc+bwUcCyzfe10REUmcsm6UOy78W6G+mNy9yMyuA6YTXOY6yd2Xmdk9QJa7704Wo4Ape10ldQTwFzMrJkhi90df/SQiIoln8V69amZtgPTd0+7+daKCqqiMjAzPyspKdhgiIjWGmc1394xY88q9zNXMRpjZ58CXwHtANvB6pUYoIiLVTjz3QdwLDAE+c/cuwEnA3IRGJSIiSRdPgih0901AipmluPtMgrurRUSkFovnPoitZtYImAU8ZWbrge2JDUtERJItnhbESCAfuAl4g+BehDMTGZSIiCRfmS2IsMO9V8IxIYqBv1VJVCIiknRltiDCri+KzaxpFcUjIiLVRDznILYBS8xsBlHnHtz9hoRFJSIiSRdPgnghfERTp3giIrVcPAmimbv/KbrAzH6WoHhERKSaiOcqpktjlI2p5DhERKSaKbUFYWYXARcDXcwsuhfWxsDmRAcmIiLJVdYhpjnAWoIR5R6IKs8DFicyKBERSb6yEsTX7v4VcHRpC5iZebzdwYqISI1S1jmImWZ2vZntMdCzmdU1sxPN7G/EPj8hIiK1QFktiGHA5cAzZtYF2ArUJ0gqbwIPu/vHiQ9RRESSoawR5QqAR4FHzSyN4FzEDnffWlXBiYhI8sRzHwTuXkhwwlpERA4Q8dwHUWFmNszMPjWzlWZ2W4z5Y8xsg5ktDB9XRM271Mw+Dx861yEiUsXiakFURNgT7J+BU4DVwDwzm+buy/da9Fl3v26vdVsAvyEYmMiB+eG6WxIVr4iI7CmeMakbmllK+PzwcIzqtDi2PQhY6e6r3H0XMIVgbIl4/BiY4e6bw6Qwg+CkuYiIVJF4DjHNAtLNrB3B1Us/ASbHsV474Juo6dVh2d7ONbPFZva8mXXYz3Uxs3FmlmVmWRs2bIgjLBERiUc8CcLcPR84B3jU3c8HelXS/l8GOrv7kQSthP0ekMjdJ7h7hrtntG7dupLCEhGRuBKEmR0NjAZeDctS41hvDdAharp9WFbC3Te5+85w8glgYLzriohIYsWTIG4EbgdedPdlZtYVmBnHevOAbmbWxczqAqOA6E7/MLO2UZMjgE/C59OBU82suZk1B04Ny0REpIqUexWTu78HvAcQnqzeGM9ocu5eZGbXEXyxpwKTwgRzD5Dl7tOAG8xsBFBE0EPsmHDdzWZ2L0GSAbjH3dWDrIhIFbLy+tozs6eBq4EIwRd2E+BP7v6/iQ9v/2RkZHhWVlaywxARqTHMbL67Z8SaF88hpp7ungucBbwOdCG4kklERGqxeBJEWnjfw1nAtLDbDXXxLSJSy8WTIP4CZAMNgVlm1gnITWRQIiKSfPGcpB4PjI8q+srMMhMXkoiIVAfxdLXR1Mwe3H23spk9QNCaEBGRWiyeQ0yTCMahviB85AJPJjIoERFJvnh6cz3U3c+Nmv5vM1uYqIBERKR6iKcFscPMjts9YWbHAjsSF5KIiFQH8bQgrgb+bmZNw+ktgAbwERGp5eK5imkR0NfMmoTTuWZ2I7A40cGJiEjyxD3kqLvnhndUA/w8QfGIiEg1UdExqa1SoxARkWqnoglCXW2IiNRypZ6DMLM8YicCA+onLCIREakWSk0Q7t64KgMREZHqpaKHmEREpJZTghARkZiUIEREJKaEJggzG2Zmn5rZSjO7Lcb8n5vZcjNbbGZvh2NN7J4XMbOF4WNaIuMUEZF9xdPVRoWYWSrwZ+AUYDUwz8ymufvyqMU+BjLcPd/MrgH+B7gwnLfD3fslKj4RESlbIlsQg4CV7r7K3XcBU4CR0Qu4+0x3zw8n5wLtExiPiIjsh0QmiHbAN1HTq8Oy0owFXo+aTg8HKJprZmclIkARESldwg4x7Q8zuwTIAE6IKu7k7mvMrCvwjpktcfcvYqw7DhgH0LFjxyqJV0TkQJDIFsQaoEPUdPuwbA9mdjLwa2CEu+/cXe7ua8K/q4B3gf6xduLuE9w9w90zWrduXXnRi4gc4BKZIOYB3cysi5nVBUYBe1yNZGb9gb8QJIf1UeXNzaxe+LwVcCwQfXJbREQSLGGHmNy9yMyuA6YDqcAkd19mZvcAWe4+DfhfoBHwLzMD+NrdRwBHAH8xs2KCJHb/Xlc/iYhIgpl77emYNSMjw7OyspIdhohIjWFm8909I9Y83UktIiIxKUGIiEhMShAiIhKTEgTw6adQVJTsKEREqpcDPkFs2gTHHAMjR8K2bcmORkSk+jjgE0TLlvD738P06XDCCbB2bbIjEhGpHg74BAEwbhxMmxYcahoyBJYtK2XB/Hx46CGYPTu+Db/7LixeXFlhiohUKSWI0Omnw6xZsGsXHHsszJwZNdMdnnsOjjgCfv5zGDYM5s0re4PTpsFJJ8HQobBqVSJDFxFJCCWIKAMGwNy50K4d/PjH8Pe/gy/4ODj2dOGF0Lw5vPACHHQQDB8OK1fG3tDs2cHy/foFyeWcc4LWh4hIDaIEsZdOHZ0PXlzPtT1nsuPSq/CBAylY+AmRR/8C8+fD2WfDG29AcXHQkli/fs8NLFkCI0ZAp07BiY2nnw4OM40bFySL2mL8+CCjfv55siOpuOJiuPrq4HXMyUl2NBW3bRuccgr89KcQiSQ7mopbtSr4UTV+fLIj+WHefht69gy+J2o6d681j4EDB/p+Kyx0/9Of3K+6yv34491btnQPvsq9KKWOT2p2ozdjs3ft6v7II+7bt4fr/ec/7vXru2dkuOflBWXZ2e6HHBI8srO/38c99wTbHD9+/+Orjp59NqiPmXvbtu6ffJLsiCrm9tu/r8dRR7lv3pzsiPZfYaH76acHdQD3iy8OymqaTZvcu3f/vh7335/siCpmyRL3Jk2CetSt6/7yy8mOqFwEfePF/E5N+sMUhOgAABM2SURBVJd6ZT4qlCCKi92bNXNv3tz9uOPcx41zf/hh9zffdF+3zouK3F94wX3w4OC/1bKl+znnuF9xhfuks6d5xFL8q96n+atPfOsFXbp7cbNmwZskWiTifuaZ7nXquM+eHX9s27e7//OfwYenvDo8/rj7wQe7X3ed+5Yt+/9/iNf777vXqxf8r7Ky3Nu0cT/oIPelSxO3z0SYMCF4QceNc3/ppeDD3L+/+8aNyY4sfsXF7ldfHdTj8cfdf/e74Pn557vv2pXs6OJXUBD8OKtb133mTPeLLgrqcc89yY5s/6xZ496hQ/CjaeHC4MdjWlrwBVKNKUGUZ9Om4MNWhuJi91mz3M89171nz+C7uG5d9ysIvmhyaOz5pPsJqbO8Z0/3885zv+su98mT3V9/3X3RrK1e2LWbFx98cPBGKs8HH7h36xa8RC1aBM2XWL8Mv/rK/eSTg+V693ZPSXFv3dp90qQgMVWmzz4LYunW7fsv0k8+CT4QrVoFH4pEiETcFyxw//rrytne66+7p6a6Dxv2/f/0tdeCxNenj/u6dZWzn73t3Ok+Z075CT9e//M/wet+663fl/3xj0HZWWcF+0uE3Nzgh05BwQ/fViTyfUJ45pmgrKjI/Sc/CcruuKPcz2aFffed+9y5lfM5yctzHzDAvWHD4L3q7r51q/uQIcF77dlnf/g+SrNypfsrr1R4dSWIBCkuDn7k5/zivz2SVtdn3jjVb7/dfeRI98MOC76rw6NVDu69WOJ5NPSP6hztwzot90GD3E891f3CC4MjXLfc4v6H/97hC065xSOW4tvbdPJPf/03z83IdAff2a2nb3zqDd+0yT1na7HveOQJL27c2IsbNvQdDz/uebnFHsla4H700cEOhwxxnz8/vsqsWuV+553uY8a4T5++74dm/Xr3Qw8NEsHnn+8577PP3Nu3D5JHvPsrT0FB8KV91VXBITsIPmiXXbbv/ncrLg5aNddfH7SksrL2/XJZuNC9USP3vn2DL7poM2YEhw179nRfu7Zy6rF1a/DFN2qUe9OmQT0aNw4Ob23YEHudSCR4DS67LPiCXLVq32Weey7Y1gUX7PtajR8fzBs+3H3Hjsqpx5o1QSvltNOCX0bg3q5dsK/8/Njr7Njh/vTTQQJ46KHY9f3Vr4Jt/f73e5YXFbmPHft9AqysJPHJJ8Hhq2OO+f5w1pFHBv/P0hLFpk1BPUeNcv/HP/atb2Fh8L9OSXF/9dU95+XmBq3tlJTgaEBliESCxHb77cF7FYL3VgUPLZaVINTdd2XZsQPq19+jqKAAvv0Wvvvu+0fLmf/ivH9fRKpH+LphD95peg4vp53D+/kD6LJlAROL/oteLGcCV/ILHmAbjQFnBNN4gF9wGF/wCsNJJcJpvME7ZDKWiWTTBQAzaNakmMvr/J1f5dxCs6KNzGl3AV93OJYtXQey4/C+ND64Ic2bQ30roN2HL9DxrYm0WvQObkakQWPqbM+lsF0ndl58GaljLyO9Q2vs5JPwjz8m8uY7FB11NEVFwb7q14eUFODLLyEzMzjZe999wbXCvXpBnRhDjhQUwKJFkJUFGzcG1xYXFn7/d+1amDEjOPnasGFwSdmZZwbrPP447NpF4fkXs/LCX/N5ag/q5G6m0/tP0WHGRJqsWkSkbjoAqbsK2NmzH4U/GUvamNHUK9oOQ4bgZhTNnkvRQe2IRCA1FdLTg/rw3nvBFWrt2sGdd8LgwXDYYeHMveTmBnVYuDB4vnc9Pv88uBemqAhat4YzzoCTTw4ugX7uObxBAwou+ynLT/sF3+w6iIYbv6LTO0/S4e0nqb/+a4oaNiE1Pw9zp+DYkygacwX1LjyLtCUL4MQT8YwMCl97i0haOkVFULdu8DAD/vKX4AT8iSfCddcF9TjkkNjv3XXrgsu2ly4N3se7du1ZlwULvr+su2vXoNuBgQNhwgSYNQs/+GByx93MkmOuYkN+Q1p8vZBOb02k3btPkbZtC4WNm5OWt4XitLoUDDsbv3ws6cNPIvVvk+DKK/ErrqTwkb9QFDEikeC1SEsjuIjguuvgscdgzBgYNQoGDQquJtybO2RnB3F+9hns3Lnna7FjR3B14WefBcsPGBBcTNKuHTzwAKxYgffsyYZxd7Co+wVszzcOWvYOnd6eyEFzXiS1cGdJPSKNm1JwzmjsyitIH9KPlOuvhcceo/jPj1E49mqKioLQGzQI3lts3x68f999F+64A049Nfj/7fV9AQQXGaxYAR99BN98s+97KicH3nor+DJJTYUf/Sh4PUaMgC5dYr++5Siru28liGRYswZeegn+/e/gCykSgQ4d4Ntv8TYHseWPE9kwcBhbtwbvh4KC4LErbyeHvf5/9H/1XlIiRcwa/gcWHvPT8Bs6eFPm5cHWrcGjcMNWzll8N5nrp9CqaB0AEVJYQQ8+pTtDeZcWbOFLOjOJy5nMGNbThpG8xBU8wanMoBgjm850Jpvz+RcvcO4eVTELvsMbNYLu6V8x6bvT6FrwCQA7UhqwrP5RLG4whG/rduLwHYvonT+PbjuXkOaFJduIWCqRlDQiqXUpTk2jML0JXx52Ciu6j2RlxxMpIJ1IJPhMbF7+HcOWPcCl+Y9Snx18wLEcxTzS2cl8BjCRsTzNxQBczNNcwRMM4GMKqMcmWtKYPI7jfZZw5B71SEkJ6tC4MRyf8gGPrj2L5kUbAdiS2pKlDQezpP5g8lKb0TM/i1475tF556ek8P3np8jqlNTD66Sxo1EbPu8+nBWHjyT74CEUeSq7dsHq1VC05BPO/+w+zi18hp3UYwEDOIY5AMzgFCYylpcYSRvWM4bJjGUinfmKzTTHcDbQmmOYwyZa7VGPOnWCOjRqBD+JPMmda39KuhcAsDatA0saDmFJ/cGkehG98ufRe8dHtC38Zo9tFFoaxSlpROrUxVPTyGnZlU+7j+CTbiP4rmUviiJGQUHwfdx04Xv811f3kFn8DutpzWral/y/X+AcJjKWmWTSm6WMZSKX8E9asplsOtGe1czgFM7kZSJ7jV1Wr174ejRy7tp+C5dufKDkf/1lvR4sbjiEZekZtChaR+/8efTckUWLyMY965FSl0hKGsWpdSmuk8aGtn1Z0WMkK7qNYHPDDhQVBb9Bsr+IcOjC57ly7b30Yhmfcjh12UUXstlMc55iNBMZy2KO5ATeYywTOY/nSWcnKzmUw/iCP3ALt/GHfT7q9esHr0erBvk8uvlCTsh9JXivUIfP6vdlSYPBfJ7ehw47V9Irfx49C+bToHh7yfrFGEUpweciklqX4rR0Vnc6lk97jOTTrqeRl9aCoqLgM3jnnfvsPi5KENXZxo3w8stBwmjbFn73u9i/kKJt3hz8QmrbNr59uAdNmfnzKZw7n8hH80lZsYxtvY9m/Zlj2do/k4inUFQUbHbbtrBfquxsDp31JIcvfJYPB15L1tHXk5oafAnVqRMkpO3bg2Xz8sL18pzW276kx9a59Mj5kO5b59I152PSvJDtdZqysnkGXzQ/ii9aHMXK5kex2tuxaUsKW7YE1dqyZd8rNVNTg0erVtCtW/CDvs/BG/jx8gfp9PFU8gafzPozx5LXtR+RCCX1yMsLHvU/WUC3WRPp9NmbvDLsz2QffmpJHVJTg+W3b4+qwzbIz4vQIW85R+TMpfvWoB4d8paTgrO53sF80TyI/4sWR7GyWQZrd7Zg89aUkjps3brvVc2793nIIUEdDjsMMpp8xskf/Y5WX81ny0nnsf70Mexo04mioiCuHTvC/29OMc0XvE339yfS6rulPHPhVHJaH1ZSh9TU4AdmdB3y8qAwr4BD8xbSI6xHjy1zOWhHNgDfNejK5y0GlbweXzTux7rtjdi8xUpej7y8fd9OdeoELZVOnb6vx/EpH3Dc7N/RIH8jm077CetPvphdjVoQiQRx5ecH28rfXMDBc6fSY85ErKiIp0dNo7hh45L3VUpK8GMoug7btoHl5dI9dx7dcz6kx9a5dN/yIc12rSdCCt806bXH6/Flg15syK3H5i1W8nrsfRuSWbC/+vWDRtFhh0G3Q4s5KfdFMmY9gDVqyMYzL2fj8WdTmJpeUo/dce38bgsd33+aIz6czHet+/DyyCdITUspqYfZ93Xevc62bVA/dx1H5H4YvBY5c+m2dR4NivLYlVKP7Kb9SurwRYuj+LJONzbl1Cl5LTZvDhoS0VJSgv21bRsk7IpQgpDkKigIDmN06FDS2imNe/CluPtLLzU19tGdpMjNDT7lbduWG1RxcVDt6C++alOP9euDwFq2LHfRwsLgsTu5lfPyVR33oCnWokXw87kcO3cGr8nuhFpt6hGJBIeSDjkkyLplcA/eU7uTW2V9NspKEAkbk1qkRHp68HMzDmbBsdtqqUmT4BGHlJRqXI82beJeNC0tPB9Q3ZgFPzjiVK9eAmP5IVJToXPnuBbdfc6vKiU0j5rZMDP71MxWmtltMebXM7Nnw/kfmlnnqHm3h+WfmtmPExmniIjsK2EJwsxSgT8DpwE9gYvMrOdei40Ftrj7YcBDEJzlCZcbBfQChgGPhtsTEZEqksgWxCBgpbuvcvddwBRg5F7LjAT+Fj5/HjjJzCwsn+LuO939S2BluD0REakiiTwH0Q6Ivn5uNTC4tGXcvcjMcoCWYfncvdZtF2snZjYOGBdObjOzTysYbytgY7lL1UyqW81Vm+unulUPpZ4grPEnqd19AjDhh27HzLJKO5Nf06luNVdtrp/qVv0l8hDTGiD6MoP2YVnMZcysDtAU2BTnuiIikkCJTBDzgG5m1sXM6hKcdJ621zLTgEvD5+cB74R9g0wDRoVXOXUBugEfJTBWERHZS8IOMYXnFK4DpgOpwCR3X2Zm9xB0DjUNmAj8w8xWApsJkgjhcs8By4Ei4Fp3T/RIKD/4MFU1prrVXLW5fqpbNVer7qQWEZHKU11uOBcRkWpGCUJERGI64BNEed2B1DRmNsnM1pvZ0qiyFmY2w8w+D/+W011s9WRmHcxsppktN7NlZvazsLzG18/M0s3sIzNbFNbtv8PyLmE3NCvDbmnK7tGtGjOzVDP72MxeCadrU92yzWyJmS00s6ywrMa/Lw/oBBFndyA1zWSC7kmi3Qa87e7dgLfD6ZqoCPiFu/cEhgDXhq9XbajfTuBEd+8L9AOGmdkQgu5nHgq7o9lC0D1NTfUz4JOo6dpUN4BMd+8Xdf9DjX9fHtAJgvi6A6lR3H0WwRVh0aK7NPkbcFaVBlVJ3H2tuy8In+cRfNm0oxbULxz9cVs4mRY+HDiRoBsaqKF1AzCz9sBw4Ilw2qgldStDjX9fHugJIlZ3IDG79KjhDnL3teHz74CDkhlMZQh7/u0PfEgtqV94CGYhsB6YAXwBbHX3onCRmvz+fBi4BSgOp1tSe+oGQTJ/08zmh93/QC14X9b4rjZk/7i7m1mNvrbZzBoB/wZudPdcixo1pSbXL7zXp5+ZNQNeBHokOaRKYWZnAOvdfb6ZDU12PAlynLuvMbM2wAwzWxE9s6a+Lw/0FsSB0qXHOjNrCxD+XZ/keCrMzNIIksNT7v5CWFxr6gfg7luBmcDRQLOwGxqoue/PY4ERZpZNcBj3ROBP1I66AeDua8K/6wmS+yBqwfvyQE8Q8XQHUhtEd2lyKfBSEmOpsPC49UTgE3d/MGpWja+fmbUOWw6YWX3gFIJzLDMJuqGBGlo3d7/d3du7e2eCz9g77j6aWlA3ADNraGaNdz8HTgWWUhvelwf6ndRmdjrB8dHd3YHcl+SQfhAzewYYStDd8DrgN8BU4DmgI/AVcIG7730iu9ozs+OA2cASvj+W/SuC8xA1un5mdiTBicxUgh9uz7n7PWbWleBXdwvgY+ASd9+ZvEh/mPAQ0y/d/YzaUrewHi+Gk3WAp939PjNrSU1/Xx7oCUJERGI70A8xiYhIKZQgREQkJiUIERGJSQlCRERiUoIQEZGYlCBEymFmkbCXzt2PSut0zcw6R/e8K1KdqKsNkfLtcPd+yQ5CpKqpBSFSQeEYAP8TjgPwkZkdFpZ3NrN3zGyxmb1tZh3D8oPM7MVwzIdFZnZMuKlUM/trOA7Em+Gd1JjZDeHYF4vNbEqSqikHMCUIkfLV3+sQ04VR83LcvQ/wCMEd+QD/B/zN3Y8EngLGh+XjgffCMR8GAMvC8m7An929F7AVODcsvw3oH27n6kRVTqQ0upNapBxmts3dG8UozyYY5GdV2Ingd+7e0sw2Am3dvTAsX+vurcxsA9A+ujuJsNvyGeGgMpjZrUCau//WzN4AthF0lTI1arwIkSqhFoTID+OlPN8f0f0PRfj+3OBwghEPBwDzono+FakSShAiP8yFUX//Ez6fQ9BrKcBogg4GIRh28hooGRyoaWkbNbMUoIO7zwRuBZoC+7RiRBJJv0hEylc/HOlttzfcffelrs3NbDFBK+CisOx64EkzuxnYAFwWlv8MmGBmYwlaCtcAa4ktFfhnmEQMGB+OEyFSZXQOQqSCwnMQGe6+MdmxiCSCDjGJiEhMakGIiEhMakGIiEhMShAiIhKTEoSIiMSkBCEiIjEpQYiISEz/D2Du3R7X+hzjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ySS2DTZJTsdm",
        "outputId": "32f1ec0d-3456-4e9f-93c6-e0dc8ccfa529"
      },
      "source": [
        "#bad one\n",
        "s\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ca2744df5870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#bad one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss (training and validation)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqP7UGFIU_n"
      },
      "source": [
        "# class_distributions['Kaggle'] = {'AP':Kaggle_AP,\n",
        "#                                'PA':Kaggle_PA,\n",
        "#                                'Total': [f'Total Images = {Kaggle_AP+Kaggle_PA}',\n",
        "#                                          f'AP Percentage = {Kaggle_AP/(Kaggle_AP+Kaggle_PA):.3f}',\n",
        "#                                          f'PA Percentage = {Kaggle_PA/(Kaggle_AP+Kaggle_PA):.3f}']}\n",
        "\n",
        "# class_distributions['IEEE'] = {'AP':IEEE_AP,\n",
        "#                                'PA':IEEE_PA,\n",
        "#                                'Total': [f'Total Images = {IEEE_AP+IEEE_PA}',\n",
        "#                                          f'AP Percentage = {IEEE_AP/(IEEE_AP+IEEE_PA):.3f}',\n",
        "#                                          f'PA Percentage = {IEEE_PA/(IEEE_AP+IEEE_PA):.3f}']}\n",
        "\n",
        "class_distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ffaai3GVdjc"
      },
      "source": [
        "test_dses = {\n",
        "    # 'CheXpert': cheXpert_ds[2],\n",
        "    # 'CX-Net': CXNet_ds[2],\n",
        "    # 'ActualMed': ActualMed_ds[2],\n",
        "    'Shenzhen': Shenzhen_ds[2],\n",
        "    'Montgomery': Montgomery_ds[2],\n",
        "    'RICORD': RICORD_ds[2],\n",
        "    'Kaggle': Kaggle_Test_ds[2]\n",
        "}\n",
        "\n",
        "predictions = {}"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSJHOlCwT8Mm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "70931736-667d-403d-906a-94624c02c725"
      },
      "source": [
        "#adapted from https://ecode.dev/cnn-for-medical-imaging-using-tensorflow-2/\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "for test_ds_name in test_dses:\n",
        "  print('-'*40)\n",
        "  print('Now evaluating '+ test_ds_name)\n",
        "  test_ds = test_dses[test_ds_name]\n",
        "  score_test = model.evaluate(test_ds)\n",
        "  predictions[test_ds_name] = model.predict(test_ds)\n",
        "  for name, value in zip(model.metrics_names, score_test):\n",
        "    print(name, ': ', value)\n",
        "  print('-'*40)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "\n",
        "# notice the threshold\n",
        "def plot_cm(labels: numpy.ndarray, predictions: numpy.ndarray, p: float=0.5) -> ():\n",
        "    cm = confusion_matrix(labels, predictions > p)\n",
        "    # you can normalize the confusion matrix\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    print('Lesions Detected (True Negatives): ', cm[0][0])\n",
        "    print('Lesions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "    print('No-Lesions Missed (False Negatives): ', cm[1][0])\n",
        "    print('No-Lesions Detected (True Positives): ', cm[1][1])\n",
        "    print('Total Lesions: ', np.sum(cm[1]))\n",
        "\n",
        "def plot_roc(name: str, labels: numpy.ndarray, predictions: numpy.ndarray, **kwargs) -> ():\n",
        "  fp, tp, _ = roc_curve(labels, predictions)\n",
        "  auc_roc = roc_auc_score(labels, predictions)\n",
        "  plt.plot(100*fp, 100*tp, label=name + \" (\" + str(round(auc_roc, 3)) + \")\", \n",
        "           linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.title('ROC curve')\n",
        "  plt.grid(True)\n",
        "  plt.legend(loc='best')\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "for name, test_ds, y_test_pred in zip(test_dses.keys, test_dses.values, predictions.values):\n",
        "  print('-'*40)\n",
        "  print('NOW SHOWING PLOTS FOR: ' + name)\n",
        "  y_test = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "  plot_cm(y_test, y_test_pred)\n",
        "  plot_roc(\"Test Base\", y_test, y_test_pred, color=colors[0])\n",
        "  plt.legend(loc='lower right')\n",
        "  print('-'*40)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "----------------------------------------\n",
            "Now evaluating Shenzhen\n",
            "27/42 [==================>...........] - ETA: 1:00 - loss: 0.3057 - tp: 387.0000 - fp: 0.0000e+00 - tn: 0.0000e+00 - fn: 45.0000 - accuracy: 0.8958 - precision: 1.0000 - recall: 0.8958 - auc: 0.0000e+00 - sensitivity: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-3051a494f172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Now evaluating '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtest_ds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_ds_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mscore_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_ds_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n  (1) Invalid argument:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[assert_greater_equal_4/Assert/AssertGuard/else/_1413/assert_greater_equal_4/Assert/AssertGuard/Assert/data_4/_166]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_140334]\n\nFunction call stack:\ntest_function -> test_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q62DZoGIb3bn"
      },
      "source": [
        "class_distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHRwEBbKSv7z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}